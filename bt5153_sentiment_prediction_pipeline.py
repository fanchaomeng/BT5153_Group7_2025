# -*- coding: utf-8 -*-
"""BT 5153 Sentiment Prediction Pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/fanchaomeng/9095451eac2b4f878da3f2e2e52dfc8d/5153project.ipynb

# **1. Install Necessary Package**
"""

# ============================================================================
# Force reinstall datasets cleanly (Something wrong on my end, need
# to unstall the first 'dataset' package and use the second. Not
# sure if others need or not.)
# ============================================================================
!pip uninstall -y datasets

!pip install datasets

!pip install transformers==4.17

"""# **2. Load and Preprocess Dataset**"""

from google.colab import drive
drive.mount('/content/gdrive')

# Loading Package
import os
import re
import matplotlib.pyplot as plt
import csv
import uuid
import nltk
import json
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from tqdm import tqdm
import seaborn as sns
from bs4 import BeautifulSoup
from collections import Counter
from wordcloud import WordCloud
from nltk.corpus import stopwords
from collections import defaultdict
from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from transformers import EarlyStoppingCallback
from sklearn.metrics import classification_report
from transformers import AutoTokenizer, AutoModel
from sklearn.linear_model import LogisticRegression
from transformers import TrainingArguments, Trainer
from torch.utils.data import Dataset as TorchDataset
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import AutoModelForSequenceClassification
from datasets import Dataset, load_dataset, get_dataset_config_names
from sklearn.metrics import precision_recall_fscore_support, accuracy_score
from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report

# Data Saving. Only run once (Delete previous file in Google Doc if you need to rerun.)
path_to_file = '/content/gdrive/My Drive'
# Create the target directory if it doesn't exist
os.makedirs("/content/gdrive/My Drive/filtered_reviews", exist_ok=True)
TARGET_PER_CLASS = 5000  # max number of reviews per sentiment
MAX_TOTAL = TARGET_PER_CLASS * 3

def get_sentiment_class(rating):
    if rating in [1, 2]:
        return "negative"
    elif rating == 3:
        return "neutral"
    elif rating in [4, 5]:
        return "positive"
    return None

# Load all raw_review configs
all_configs = get_dataset_config_names("McAuley-Lab/Amazon-Reviews-2023")
raw_review_configs = [cfg for cfg in all_configs if cfg.startswith("raw_review")]

for config_name in raw_review_configs:
    print(f"üîÑ Processing {config_name}...")

    dataset = load_dataset(
        "McAuley-Lab/Amazon-Reviews-2023",
        config_name,
        split="full",
        streaming=True,
        trust_remote_code=True
    )

    output_path = f"{path_to_file}/filtered_reviews/{config_name}.csv"
    with open(output_path, mode="w", newline='', encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([
            "asin", "title", "text", "rating", "sentiment",
            "user_id", "timestamp", "verified_purchase", "helpful_vote"
        ])

        class_counts = defaultdict(int)

        for item in dataset:
            try:
                rating = int(round(float(item.get("rating", -1))))
            except:
                continue

            sentiment = get_sentiment_class(rating)
            if sentiment is None or class_counts[sentiment] >= TARGET_PER_CLASS:
                continue

            asin = item.get("asin", item.get("parent_asin", ""))
            title = item.get("title", "") or item.get("summary", "")
            text = item.get("text", "") or item.get("reviewText", "")
            user_id = item.get("user_id", "")
            timestamp = item.get("timestamp", "")
            verified = item.get("verified_purchase", "")
            helpful = item.get("helpful_vote", 0)

            writer.writerow([
                asin, title, text, rating, sentiment,
                user_id, timestamp, verified, helpful
            ])

            class_counts[sentiment] += 1

            if sum(class_counts.values()) >= MAX_TOTAL:
                break

        print(f"‚úÖ Saved {sum(class_counts.values())} reviews from {config_name}: {dict(class_counts)}")

# Data Cleaning & Balanced Only Run Once (Delete previous file in Google Doc if you need to rerun.)
# Path to your group CSVs
input_dir = "/content/gdrive/My Drive/filtered_reviews"
output_path = "/content/gdrive/My Drive/filtered_reviews/final_balanced_reviews.csv"

# Storage for all sampled subsets
all_data = []

# Loop through all group CSV files
for file in os.listdir(input_dir):
    if not file.endswith(".csv") or file == "combined_reviews.csv":
        continue

    file_path = os.path.join(input_dir, file)
    print(f"üì• Loading {file}...")

    # Read the group file
    df = pd.read_csv(file_path)

    # Skip if required columns are missing
    if not {'text', 'sentiment', 'title'}.issubset(df.columns):
        print(f"‚ö†Ô∏è Skipping {file} ‚Äî missing columns.")
        continue

    # Drop missing
    df = df[['title', 'text', 'sentiment']].dropna()

    # Sample 1000 per class
    try:
        df_sampled = df.groupby('sentiment', group_keys=False).apply(
            lambda x: x.sample(n=1000, random_state=42)
        ).reset_index(drop=True)

        all_data.append(df_sampled)
    except:
        print(f"‚ö†Ô∏è Not enough data in {file}, skipping...")

# Combine all sampled data
final_df = pd.concat(all_data, ignore_index=True)

# Create model input by combining title and text
final_df['input'] = final_df['title'].str.strip() + ' ' + final_df['text'].str.strip()

# Encode sentiment to numerical label
label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
final_df['label'] = df['sentiment'].map(label_map)

# Add unique review_id column
final_df['review_id'] = [str(uuid.uuid4()) for _ in range(len(final_df))]

# Move review_id to first column
cols = ['review_id'] + [col for col in final_df.columns if col != 'review_id']
final_df = final_df[cols]

# Save to CSV (including input, title, text, sentiment, label)
final_df.to_csv(output_path, index=False, encoding="utf-8")

print(f"Final dataset saved: {output_path}")
print(final_df['sentiment'].value_counts())

df = pd.read_csv("/content/gdrive/My Drive/filtered_reviews/final_balanced_reviews.csv")
print("Total samples:", len(df))
print("Class distribution:\n", df['sentiment'].value_counts())
print(df.head(10))

"""# **3. EDA**"""

# ==========================================
# 1. Load Sample of True Raw Data (~100K)
# ==========================================
# Settings
MAX_RECORDS = 100_000

def get_sentiment_class(rating):
    try:
        rating = int(round(float(rating)))
        if rating in [1, 2]:
            return "negative"
        elif rating == 3:
            return "neutral"
        elif rating in [4, 5]:
            return "positive"
    except:
        return None

# Get available configs (like raw_review_Books, raw_review_Toys, etc.)
all_configs = get_dataset_config_names("McAuley-Lab/Amazon-Reviews-2023")
raw_review_configs = [cfg for cfg in all_configs if cfg.startswith("raw_review")]

# Storage for all records
records = []
total_count = 0

# Loop over datasets
for config_name in raw_review_configs:
    print(f"üîÑ Streaming from {config_name}...")

    dataset = load_dataset(
        "McAuley-Lab/Amazon-Reviews-2023",
        config_name,
        split="full",
        streaming=True,
        trust_remote_code=True
    )

    for item in tqdm(dataset, desc=f"Collecting from {config_name}"):
        rating = item.get("rating", None)
        sentiment = get_sentiment_class(rating)
        if sentiment is None:
            continue

        records.append({
            "asin": item.get("asin", item.get("parent_asin", "")),
            "title": item.get("title", "") or item.get("summary", ""),
            "text": item.get("text", "") or item.get("reviewText", ""),
            "rating": rating,
            "sentiment": sentiment,
            "user_id": item.get("user_id", ""),
            "timestamp": item.get("timestamp", ""),
            "verified_purchase": item.get("verified_purchase", ""),
            "helpful_vote": item.get("helpful_vote", 0)
        })

        total_count += 1
        if total_count >= MAX_RECORDS:
            break
    if total_count >= MAX_RECORDS:
        break

# Convert to DataFrame
raw_df = pd.DataFrame(records)
print("‚úÖ Loaded shape:", raw_df.shape)
print(f"\n‚úÖ Loaded {len(raw_df)} samples into memory.")

raw_df.head()

# =======================
# Sentiment Distribution
# =======================
sns.countplot(data=raw_df, x="sentiment", order=["positive", "neutral", "negative"])
plt.title("Sentiment Distribution (Raw Sample)")
plt.xlabel("Sentiment")
plt.ylabel("Count")
plt.show()

# =======================
# Text Length Distribution
# =======================
raw_df["text_length"] = raw_df["text"].astype(str).apply(len)

sns.histplot(data=raw_df, x="text_length", bins=100)
plt.xlim(0, 5000)  # Limit to ignore extreme outliers
plt.title("Text Length Distribution (Raw Sample)")
plt.xlabel("Text Length (characters)")
plt.ylabel("Count")
plt.show()

# =======================
# Helpful Votes by Sentiment
# =======================
sns.boxplot(data=raw_df, x="sentiment", y="helpful_vote", order=["positive", "neutral", "negative"])
plt.yscale("log")  # Log scale due to skew
plt.title("Helpful Votes Distribution by Sentiment (Raw Sample)")
plt.xlabel("Sentiment")
plt.ylabel("Helpful Votes (Log Scale)")
plt.show()

# =======================================
# 2. EDA on Cleaned & Balanced Data
#=========================================
balanced_df = pd.read_csv("/content/gdrive/My Drive/filtered_reviews/final_balanced_reviews.csv")
print("Balanced dataset shape:", balanced_df.shape)
print(balanced_df['sentiment'].value_counts())

# Sentiment count
sns.countplot(data=balanced_df, x='sentiment')
plt.title("Sentiment Distribution in Balanced Dataset")
plt.show()

# Text length distribution
balanced_df['text_length'] = balanced_df['text'].astype(str).apply(len)
sns.histplot(balanced_df['text_length'], bins=50)
plt.title("Text Length Distribution (Balanced Data)")
plt.xlim(0, 5000)
plt.xlabel("Text Length (characters)")
plt.ylabel("Count")
plt.show()

# =======================================
# 3. Comparision
#=========================================
# Compare sentiment distributions side by side
# Ensure both DataFrames use consistent casing
raw_df['sentiment'] = raw_df['sentiment'].str.lower()
balanced_df['sentiment'] = balanced_df['sentiment'].str.lower()

# Count sentiment classes in both datasets
raw_counts = raw_df['sentiment'].value_counts().reindex(["positive", "neutral", "negative"])
balanced_counts = balanced_df['sentiment'].value_counts().reindex(["positive", "neutral", "negative"])

# Build comparison DataFrame
comparison_df = pd.DataFrame({
    'Raw Sample (100K)': raw_counts,
    'Balanced (3K/class)': balanced_counts
}).reset_index().rename(columns={'index': 'Sentiment'})

# Plot side-by-side bar chart
comparison_df.set_index('sentiment').plot(kind='bar', figsize=(8, 5))
plt.title("Sentiment Distribution: Raw Sample vs. Balanced Dataset")
plt.ylabel("Review Count")
plt.xlabel("Sentiment Class")
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.show()

"""# **4. Tranformer (Roberta) Model**"""

# Load the full data including 'title'
df = pd.read_csv('/content/gdrive/My Drive/filtered_reviews/final_balanced_reviews.csv')

# Convert to Hugging Face Dataset (including review_id)
hf_dataset = Dataset.from_pandas(df[['review_id', 'input', 'label']])

# Split into 80% train, 10% val, 10% test
split_dataset = hf_dataset.train_test_split(test_size=0.2, seed=42)
val_test = split_dataset['test'].train_test_split(test_size=0.5, seed=42)

# Final dataset dict
dataset = {
    'train': split_dataset['train'],
    'val': val_test['train'],
    'test': val_test['test']
}

print(f"Dataset split complete: Train={len(dataset['train'])}, Val={len(dataset['val'])}, Test={len(dataset['test'])}")

# Tokenize reviews
# checkpoint = "roberta-base"
checkpoint = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

def tokenize_function(examples):
    return tokenizer(
        examples["input"],
        padding="max_length",
        truncation=True,
        max_length=512
    )

# Tokenize all splits
tokenized_datasets = {
    split: dataset[split].map(tokenize_function, batched=True)
    for split in ['train', 'val', 'test']
}

# Confirm GPU available and print GPU info
if torch.cuda.is_available():
    num_gpus = torch.cuda.device_count()
    gpu_name = torch.cuda.get_device_name(0)
    print(f"‚úÖ GPU available: {num_gpus} GPU(s) detected.")
    print(f"üñ•Ô∏è GPU Name: {gpu_name}")
    device = torch.device("cuda")
else:
    print("‚ö†Ô∏è GPU not available, using CPU.")
    device = torch.device("cpu")

# Load model to the selected device
checkpoint = "roberta-base"
# Load model with 3 output labels
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3).to(device)

# Function for Evaluation
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    preds = predictions.argmax(axis=1)

    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)

    return {
        'accuracy': acc,
        'precision': precision,
        'recall': recall,
        'f1': f1,
    }

training_args = TrainingArguments(
    output_dir="/content/gdrive/My Drive/filtered_reviews/review_classifier_output",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.05,
    logging_dir="./logs",
    load_best_model_at_end=True,
    report_to="none"  # disable wandb reporting
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["val"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

trainer.train()

test_results = trainer.evaluate(tokenized_datasets['test'])
print("Final Test Set Evaluation:")
print(test_results)

# Check if 'test' balanced
print(Counter(dataset['test']['label']))

# Check the output ratio
predictions = trainer.predict(tokenized_datasets['test'])
print("Prediction label counts:", np.bincount(predictions.predictions.argmax(axis=1)))

# Check if input_ids look normal
tokenized_datasets['test'][0]

# Run prediction on the test set
predictions_output = trainer.predict(tokenized_datasets['test'])

# Get predicted labels
predicted_labels = predictions_output.predictions.argmax(axis=1)

# Get true labels and review_ids
true_labels = tokenized_datasets['test']['label']
review_ids = tokenized_datasets['test']['review_id']

# Build output records
output_records = []
for i in range(len(predicted_labels)):
    output_records.append({
        "review_id": review_ids[i],
        "true_label": int(true_labels[i]),
        "transformer_pred": int(predicted_labels[i])
    })

# Save to JSON
output_json_path = "/content/gdrive/My Drive/filtered_reviews/transformer_predictions.json"
with open(output_json_path, "w", encoding="utf-8") as f:
    for record in output_records:
        json.dump(record, f)
        f.write("\n")

print(f"Transformer predictions saved to: {output_json_path}")

"""# **5. Data processing for Logistic Regression**"""

# Save as CSV, if need
df_output = pd.DataFrame(output_records)
output_csv_path = "/content/gdrive/My Drive/filtered_reviews/transformer_predictions.csv"
df_output.to_csv(output_csv_path, index=False)
print(f"Transformer predictions also saved to: {output_csv_path}")

# === Stopword Removal + WordCloud Visualization (for insight) ===

# Download NLTK stopwords
nltk.download('stopwords')
nltk_stopwords = set(stopwords.words('english'))

# Combine with domain-specific stopwords
custom_stopwords = nltk_stopwords | {
    "product", "use", "one", "thing", "item", "get", "got", "would", "really", "could",
    "also", "still", "even", "though", "make", "much", "many", "buy", "bought", "review",
    "amazon", "dont", "didnt", "doesnt", "wasnt", "isnt", "arent", "wont", "cant", "im"
}

# Load shared dataset
wc_df = pd.read_csv('/content/gdrive/My Drive/filtered_reviews/final_balanced_reviews.csv')
wc_df['title'] = wc_df['title'].fillna('')
wc_df['text'] = wc_df['text'].fillna('')
wc_df['text_combined'] = wc_df['title'].str.strip() + ' ' + wc_df['text'].str.strip()

# Clean text function
def clean_for_wordcloud(text):
    text = BeautifulSoup(str(text), "html.parser").get_text()
    text = re.sub(r"[^a-zA-Z\s]", "", text).lower()
    return " ".join([w for w in text.split() if w not in custom_stopwords and len(w) > 2])

wc_df['text_cleaned'] = wc_df['text_combined'].apply(clean_for_wordcloud)

# Split
positive_texts = wc_df[wc_df['sentiment'].str.lower() == 'positive']['text_cleaned'].tolist()
negative_texts = wc_df[wc_df['sentiment'].str.lower() == 'negative']['text_cleaned'].tolist()

# TF-IDF function
def compute_tfidf_freqs(texts):
    tfidf = TfidfVectorizer(max_features=1000)
    mat = tfidf.fit_transform(texts)
    return dict(zip(tfidf.get_feature_names_out(), mat.mean(axis=0).A1))

pos_freqs = compute_tfidf_freqs(positive_texts)
neg_freqs = compute_tfidf_freqs(negative_texts)

# Plot word clouds
fig, axs = plt.subplots(1, 2, figsize=(20, 10))
wc_pos = WordCloud(width=800, height=500, background_color='white', colormap='Greens').generate_from_frequencies(pos_freqs)
wc_neg = WordCloud(width=800, height=500, background_color='white', colormap='Reds').generate_from_frequencies(neg_freqs)

axs[0].imshow(wc_pos, interpolation='bilinear')
axs[0].set_title("Positive Review Keywords (TF-IDF)", fontsize=18)
axs[0].axis('off')

axs[1].imshow(wc_neg, interpolation='bilinear')
axs[1].set_title("Negative Review Keywords (TF-IDF)", fontsize=18)
axs[1].axis('off')

plt.tight_layout()
plt.show()

# Print top keywords
print("\nüîµ Top 10 Positive Review Keywords:")
for w, s in sorted(pos_freqs.items(), key=lambda x: x[1], reverse=True)[:10]:
    print(f"{w:<15} {s:.4f}")

print("\nüî¥ Top 10 Negative Review Keywords:")
for w, s in sorted(neg_freqs.items(), key=lambda x: x[1], reverse=True)[:10]:
    print(f"{w:<15} {s:.4f}")

wc_df.head()

"""# **6. Logistic Regression**"""

# =========================================
# Logistic Regression with TF-IDF
# =========================================


# Load shared dataset
logi_df = pd.read_csv('/content/gdrive/My Drive/filtered_reviews/final_balanced_reviews.csv')
logi_df['title'] = logi_df['title'].fillna('')
logi_df['text'] = logi_df['text'].fillna('')
label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
logi_df['label'] = logi_df['sentiment'].map(label_map)

# Clean text
def clean_text_logi(text):
    text = BeautifulSoup(text, "html.parser").get_text()
    return re.sub(r"[^a-zA-Z\s]", "", text).lower()

logi_df['clean_title'] = logi_df['title'].apply(clean_text_logi)
logi_df['clean_text'] = logi_df['text'].apply(clean_text_logi)

# 80:10:10 Split (Train/Val/Test)
from sklearn.model_selection import train_test_split
X_title = logi_df['clean_title'].tolist()
X_text = logi_df['clean_text'].tolist()
y_logi = logi_df['label'].tolist()
id_logi = logi_df['review_id'].tolist()

X_temp_t, X_test_t, X_temp_x, X_test_x, y_temp, y_test_logi, _, id_test_logi = train_test_split(
    X_title, X_text, y_logi, id_logi, test_size=0.1, random_state=42, stratify=y_logi)
X_train_t, X_val_t, X_train_x, X_val_x, y_train_logi, y_val_logi = train_test_split(
    X_temp_t, X_temp_x, y_temp, test_size=0.1111, random_state=42, stratify=y_temp)

# TF-IDF Vectorization
title_vec = TfidfVectorizer(max_features=3000)
text_vec = TfidfVectorizer(max_features=5000)

X_train_logi = np.hstack([
    title_vec.fit_transform(tqdm(X_train_t, desc="TF-IDF title train")).toarray(),
    text_vec.fit_transform(tqdm(X_train_x, desc="TF-IDF text train")).toarray()
])

X_val_logi = np.hstack([
    title_vec.transform(tqdm(X_val_t, desc="TF-IDF title val")).toarray(),
    text_vec.transform(tqdm(X_val_x, desc="TF-IDF text val")).toarray()
])

X_test_logi = np.hstack([
    title_vec.transform(tqdm(X_test_t, desc="TF-IDF title test")).toarray(),
    text_vec.transform(tqdm(X_test_x, desc="TF-IDF text test")).toarray()
])

# Train & Predict
clf_logi = LogisticRegression(max_iter=1000)
clf_logi.fit(X_train_logi, y_train_logi)
y_pred_logi = clf_logi.predict(X_test_logi)

# Evaluate on Test Set
print("\nüìä Logistic Regression Evaluation:")
print(f"‚úÖ Accuracy: {accuracy_score(y_test_logi, y_pred_logi):.4f}")
print(f"üéØ Macro F1 Score: {f1_score(y_test_logi, y_pred_logi, average='macro'):.4f}")
print(f"üìâ Negative Recall: {recall_score(y_test_logi, y_pred_logi, average=None)[0]:.4f}")
print("\nüîç Classification Report:\n", classification_report(y_test_logi, y_pred_logi, target_names=['negative', 'neutral', 'positive']))

# Export
pd.DataFrame({
    'review_id': id_test_logi,
    'true_label': y_test_logi,
    'logistic_pred': y_pred_logi
}).to_json("/content/gdrive/My Drive/filtered_reviews/logistic_predictions.json", orient="records", lines=True)

print("\n Logistic Regression predictions saved.")

"""# **7. Bi-LSTM**"""

# =========================================
# Bi-LSTM with RoBERTa Embeddings and Early Stopping (80:10:10 Split)
# =========================================

# Load dataset
lstm_df = pd.read_csv('/content/gdrive/My Drive/filtered_reviews/final_balanced_reviews.csv')
lstm_df['title'] = lstm_df['title'].fillna('')
lstm_df['text'] = lstm_df['text'].fillna('')
label_map = {'negative': 0, 'neutral': 1, 'positive': 2}
lstm_df['label'] = lstm_df['sentiment'].map(label_map)

# Define GPU device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Split
X_title_lstm = lstm_df['title'].tolist()
X_text_lstm = lstm_df['text'].tolist()
y_lstm = lstm_df['label'].tolist()
id_lstm = lstm_df['review_id'].tolist()

X_temp_t, X_test_t, X_temp_x, X_test_x, y_temp_lstm, y_test_lstm, _, id_test_lstm = train_test_split(
    X_title_lstm, X_text_lstm, y_lstm, id_lstm, test_size=0.1, random_state=42, stratify=y_lstm)
X_train_t, X_val_t, X_train_x, X_val_x, y_train_lstm, y_val_lstm = train_test_split(
    X_temp_t, X_temp_x, y_temp_lstm, test_size=0.1111, random_state=42, stratify=y_temp_lstm)

# Dataset
tokenizer_lstm = AutoTokenizer.from_pretrained("roberta-base")
tokenizer_lstm.pad_token = tokenizer_lstm.eos_token
max_len = 64
class RobertaDataset(TorchDataset):
    def __init__(self, titles, texts, labels):
        self.pairs = [f"{a} {b}" for a, b in zip(titles, texts)]
        self.labels = labels
    def __getitem__(self, idx):
        enc = tokenizer_lstm(self.pairs[idx], truncation=True, padding='max_length', max_length=max_len, return_tensors='pt')
        return {
            'input_ids': enc['input_ids'].squeeze(),
            'attention_mask': enc['attention_mask'].squeeze(),
            'label': torch.tensor(self.labels[idx])
        }
    def __len__(self):
        return len(self.labels)

train_dl = DataLoader(RobertaDataset(X_train_t, X_train_x, y_train_lstm), batch_size=16, shuffle=True)
val_dl = DataLoader(RobertaDataset(X_val_t, X_val_x, y_val_lstm), batch_size=16)
test_dl = DataLoader(RobertaDataset(X_test_t, X_test_x, y_test_lstm), batch_size=16)

# Model
class RobertaBiLSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.roberta = AutoModel.from_pretrained("roberta-base")
        self.lstm = nn.LSTM(self.roberta.config.hidden_size, 128, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(256, 3)
    def forward(self, input_ids, attention_mask):
        with torch.no_grad():
            x = self.roberta(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        _, (h, _) = self.lstm(x)
        h = torch.cat((h[-2], h[-1]), dim=1)
        return self.fc(h)

model_lstm = RobertaBiLSTM().to(device)
opt = torch.optim.Adam(model_lstm.parameters(), lr=2e-4)
crit = nn.CrossEntropyLoss()

# Train
best_f1 = 0; stop = 0
for _ in tqdm(range(10), desc="Training LSTM"):
    model_lstm.train()
    for batch in train_dl:
        opt.zero_grad()
        out = model_lstm(batch['input_ids'].to(device), batch['attention_mask'].to(device))
        loss = crit(out, batch['label'].to(device))
        loss.backward()
        opt.step()
    model_lstm.eval()
    pred, lab = [], []
    with torch.no_grad():
        for batch in val_dl:
            o = model_lstm(batch['input_ids'].to(device), batch['attention_mask'].to(device))
            pred += torch.argmax(o, dim=1).cpu().tolist()
            lab += batch['label'].tolist()
    f1 = f1_score(lab, pred, average='macro')
    if f1 > best_f1:
        best_f1 = f1
        best_model = model_lstm.state_dict()
        stop = 0
    else:
        stop += 1
        if stop >= 2:
            print("‚èπÔ∏è Early stopping triggered")
            break

# Predict
model_lstm.load_state_dict(best_model)
model_lstm.eval()
test_preds_lstm = []
with torch.no_grad():
    for batch in tqdm(test_dl, desc="Testing LSTM"):
        o = model_lstm(batch['input_ids'].to(device), batch['attention_mask'].to(device))
        test_preds_lstm += torch.argmax(o, dim=1).cpu().tolist()

# Evaluate on Test Set
print("\nüìä LSTM Evaluation:")
print(f"‚úÖ Accuracy: {accuracy_score(y_test_lstm, test_preds_lstm):.4f}")
print(f"üéØ Macro F1 Score: {f1_score(y_test_lstm, test_preds_lstm, average='macro'):.4f}")
print(f"üìâ Negative Recall: {recall_score(y_test_lstm, test_preds_lstm, average=None)[0]:.4f}")
print("\nüîç Classification Report:\n", classification_report(y_test_lstm, test_preds_lstm, target_names=['negative', 'neutral', 'positive']))

# Export
pd.DataFrame({
    'review_id': id_test_lstm,
    'true_label': [int(v) for v in y_test_lstm],
    'lstm_pred': [int(v) for v in test_preds_lstm]
}).to_json("/content/gdrive/My Drive/filtered_reviews/lstm_predictions.json", orient="records", lines=True)

"""# **8. Majority Voting**"""

# Load predictions
logi_df = pd.read_json("/content/gdrive/My Drive/filtered_reviews/logistic_predictions.json", lines=True)
lstm_df = pd.read_json("/content/gdrive/My Drive/filtered_reviews/lstm_predictions.json", lines=True)
trans_df = pd.read_json("/content/gdrive/My Drive/filtered_reviews/transformer_predictions.json", lines=True)

# Rename prediction columns for consistency
logi_df.rename(columns={"logistic_pred": "logi_pred"}, inplace=True)
lstm_df.rename(columns={"lstm_pred": "lstm_pred"}, inplace=True)
trans_df.rename(columns={"transformer_pred": "trans_pred"}, inplace=True)

# Merge on review_id
merged = logi_df.merge(lstm_df, on=["review_id", "true_label"]).merge(trans_df, on=["review_id", "true_label"])

# Majority Voting
def majority_vote(row):
    votes = [row["logi_pred"], row["lstm_pred"], row["trans_pred"]]
    vote_counts = Counter(votes)
    return vote_counts.most_common(1)[0][0]  # most frequent label

merged["majority_pred"] = merged.apply(majority_vote, axis=1)

# Evaluate
from sklearn.metrics import classification_report, accuracy_score, f1_score

print("\nüìä Majority Voting Evaluation:")
print(f"‚úÖ Accuracy: {accuracy_score(merged['true_label'], merged['majority_pred']):.4f}")
print(f"üéØ Macro F1 Score: {f1_score(merged['true_label'], merged['majority_pred'], average='macro'):.4f}")
print("\nüîç Classification Report:\n", classification_report(merged['true_label'], merged['majority_pred'], target_names=['negative', 'neutral', 'positive']))

# Save to JSON
merged.to_json("/content/gdrive/My Drive/filtered_reviews/majority_voting_predictions.json", orient="records", lines=True)
print("‚úÖ Saved to: majority_voting_predictions.json")