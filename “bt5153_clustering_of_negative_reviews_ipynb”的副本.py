# -*- coding: utf-8 -*-
"""â€œBT5153_Clustering_of_Negative_Reviews.ipynbâ€çš„å‰¯æœ¬

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14_I-qT9omJgFs6-_qQqZ2pOBXxz4iBRH
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

json_path = '/content/drive/MyDrive/transformer_predictions.json'
df_test = pd.read_json(json_path, lines=True)

# æŸ¥çœ‹æ•°æ®ç»“æ„
print(df_test.info())
print(df_test.head())

df_reviews = pd.read_csv('/content/drive/MyDrive/final_balanced_reviews.csv')

print(df_reviews.info())
print(df_reviews.head())

df_merged = pd.merge(df_test, df_reviews, on='review_id', how='left')
df_final = df_merged[['review_id', 'text','title', 'transformer_pred']]
df_final = df_final[df_final['transformer_pred'] == 0]
df = df_final

!pip install kneed

df_final.to_csv("/content/drive/MyDrive/5153_negative_reviews.csv")

"""## Only text TF-IDF + K-means"""

from sklearn.metrics import silhouette_score
import pandas as pd
import glob
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import pairwise_distances_argmin_min
from kneed import KneeLocator

# 1. åŠ è½½æ•°æ®
df = df_final

# 2. å‡†å¤‡å­—æ®µï¼ˆåªä¿ç•™ textï¼‰
df["text"] = df["text"].fillna("").astype(str)
df = df[df["text"].str.len() > 20].reset_index(drop=True)

# 3. ç”¨ text åš TF-IDF å‘é‡åŒ–
vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, stop_words='english', max_features=5000)
X_text = vectorizer.fit_transform(df["text"])

# 4. Elbow æ³• + Silhouette Score æ‰¾æœ€ä½³ K
inertias = []
silhouettes = []
ks = range(2, 15)
print("\nğŸ“ˆ Calculating inertia & silhouette for different K values...")
for k in ks:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X_text)
    inertias.append(km.inertia_)
    score = silhouette_score(X_text, labels)
    silhouettes.append(score)
    print(f"K={k}, Inertia={round(km.inertia_, 2)}, Silhouette={round(score, 4)}")

# åŒè½´å›¾ï¼šInertia + Silhouette
fig, ax1 = plt.subplots(figsize=(10, 5))

color1 = "tab:blue"
ax1.set_xlabel("Number of Clusters (K)")
ax1.set_ylabel("Inertia", color=color1)
ax1.plot(ks, inertias, marker='o', color=color1)
ax1.tick_params(axis="y", labelcolor=color1)

ax2 = ax1.twinx()
color2 = "tab:green"
ax2.set_ylabel("Silhouette Score", color=color2)
ax2.plot(ks, silhouettes, marker='s', linestyle='--', color=color2)
ax2.tick_params(axis="y", labelcolor=color2)

plt.title("Elbow Method + Silhouette Score (Text Only)")
plt.grid(True)
plt.show()

# æ‹ç‚¹æ£€æµ‹ï¼ˆä»ç”¨ Inertia æ‰¾ best_kï¼‰
from kneed import KneeLocator
kl = KneeLocator(ks, inertias, curve="convex", direction="decreasing")
best_k = kl.elbow
print(f"\nâœ… Optimal K from Elbow: {best_k}")

# 5. èšç±»
model = KMeans(n_clusters=best_k, random_state=42)
df["cluster"] = model.fit_predict(X_text)

# 6. æ¯ä¸ª cluster çš„å…³é”®è¯ï¼ˆå–å‰ 10 ä¸ªï¼‰
terms = vectorizer.get_feature_names_out()
order_centroids = model.cluster_centers_.argsort()[:, ::-1]

print("\nğŸ“Œ æ¯ä¸ªç°‡çš„å‰ 10 ä¸ªå…³é”®è¯ï¼ˆåªç”¨ textï¼‰ï¼š\n")
for i in range(best_k):
    top_words = [terms[ind] for ind in order_centroids[i, :10]]
    print(f"Cluster {i}: {', '.join(top_words)}")

# 7. æ¯ä¸ª cluster çš„ä»£è¡¨è¯„è®ºï¼ˆæœ€é è¿‘ä¸­å¿ƒï¼‰
print("\nğŸ“ æ¯ä¸ª cluster çš„ä»£è¡¨è¯„è®ºï¼š\n")
closest, _ = pairwise_distances_argmin_min(model.cluster_centers_, X_text)
for i, idx in enumerate(closest):
    print(f"\nCluster {i} Example Review:\n{df.iloc[idx]['text'][:400]}...\n")

# 8. å¯è§†åŒ–é™ç»´
svd = TruncatedSVD(n_components=2, random_state=42)
X_svd = svd.fit_transform(X_text)

plt.figure(figsize=(10, 6))
plt.scatter(X_svd[:, 0], X_svd[:, 1], c=df["cluster"], cmap="tab10", s=10)
plt.title(f"KMeans Cluster Visualization (Text Only, K={best_k})")
plt.xlabel("SVD 1")
plt.ylabel("SVD 2")
plt.grid(True)
plt.show()

# 9. æ¯ä¸ª cluster æŠ½æ · 50 æ¡æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
sampled_df = df.groupby("cluster").apply(lambda x: x.sample(n=min(50, len(x)), random_state=42)).reset_index(drop=True)
sampled_df.to_csv("text_only_cluster_samples.csv", index=False)
print("\nğŸ“¦ Sampled 50 comments per cluster saved to: text_only_cluster_samples.csv")

"""## textå’Œtitleåˆ†å¼€encoding, TF-IDF + K-means"""

import pandas as pd
import glob
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import pairwise_distances_argmin_min, silhouette_score  # âœ… åŠ ä¸Š silhouette_score
from scipy.sparse import hstack
from kneed import KneeLocator

df = df_final

df["title"] = df["title"].fillna("").astype(str)
df["text"] = df["text"].fillna("").astype(str)
df = df[df["text"].str.len() > 20].reset_index(drop=True)

# 2. åˆ†å¼€å‘é‡åŒ– title å’Œ textï¼ˆä¿ç•™é«˜é¢‘é‡è¦è¯ï¼‰
vectorizer_title = TfidfVectorizer(max_df=0.8, min_df=3, stop_words='english', max_features=2000)
vectorizer_text = TfidfVectorizer(max_df=0.9, min_df=5, stop_words='english', max_features=5000)

X_title = vectorizer_title.fit_transform(df["title"])
X_text = vectorizer_text.fit_transform(df["text"])

# 3. åˆå¹¶ title å’Œ text çš„ç‰¹å¾
X_combined = hstack([X_title, X_text])

# 4. Elbow æ³•æ‰¾æœ€ä½³ K
inertias = []
silhouette_scores = []  # âœ… å­˜å‚¨æ¯ä¸ª k çš„è½®å»“ç³»æ•°
ks = range(2, 15)
print("\nğŸ“ˆ Calculating inertia & silhouette for different K values...")
for k in ks:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X_combined)
    inertia = km.inertia_
    inertias.append(inertia)

    # âœ… è®¡ç®— silhouetteï¼ˆæ ·æœ¬æ•° >= k+1ï¼Œå¦åˆ™å¯èƒ½æŠ¥é”™ï¼‰
    if len(df) > k:
        score = silhouette_score(X_combined, labels)
        silhouette_scores.append(score)
        print(f"K={k}, Inertia={round(inertia, 2)}, Silhouette={round(score, 4)}")
    else:
        silhouette_scores.append(None)
        print(f"K={k}, Inertia={round(inertia, 2)}, Silhouette=N/A")

# ç”» Elbow å›¾ + Silhouette Score å›¾
fig, ax1 = plt.subplots(figsize=(10, 5))
ax1.plot(ks, inertias, marker='o', color='blue', label='Inertia')
ax1.set_xlabel("Number of Clusters (K)")
ax1.set_ylabel("Inertia", color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1.set_title("Elbow Method & Silhouette Score")

ax2 = ax1.twinx()
ax2.plot(ks, silhouette_scores, marker='s', color='green', label='Silhouette Score')
ax2.set_ylabel("Silhouette Score", color='green')
ax2.tick_params(axis='y', labelcolor='green')

plt.grid(True)
plt.show()

# è‡ªåŠ¨è¯†åˆ«æ‹ç‚¹
kl = KneeLocator(ks, inertias, curve="convex", direction="decreasing")
best_k = kl.elbow
print(f"\nâœ… Optimal K detected by Elbow: {best_k}")

# 5. ç”¨æœ€ä½³ K èšç±»
model = KMeans(n_clusters=best_k, random_state=42)
df["cluster"] = model.fit_predict(X_combined)

# 6. è¾“å‡ºæ¯ä¸ª cluster çš„å‰ 10 ä¸ªå…³é”®è¯ï¼ˆä»… text éƒ¨åˆ†ï¼‰
terms = vectorizer_text.get_feature_names_out()
order_centroids = model.cluster_centers_[:, X_title.shape[1]:].argsort()[:, ::-1]

print("\nğŸ“Œ æ¯ä¸ªç°‡çš„å‰ 10 ä¸ªå…³é”®è¯ï¼ˆä»…åŸºäºæ­£æ–‡ï¼‰\n")
for i in range(best_k):
    top_words = [terms[ind] for ind in order_centroids[i, :10]]
    print(f"Cluster {i}: {', '.join(top_words)}")

# 7. æ¯ä¸ª cluster çš„ä»£è¡¨è¯„è®ºï¼ˆæœ€é è¿‘ä¸­å¿ƒï¼‰
print("\nğŸ“ æ¯ä¸ª cluster çš„ä»£è¡¨è¯„è®ºï¼š\n")
closest, _ = pairwise_distances_argmin_min(model.cluster_centers_, X_combined)
for i, idx in enumerate(closest):
    print(f"\nCluster {i} Example Review:\n{df.iloc[idx]['title']} | {df.iloc[idx]['text'][:400]}...\n")

# 8. å¯è§†åŒ–ï¼šTruncatedSVD é™åˆ° 2 ç»´
svd = TruncatedSVD(n_components=2, random_state=42)
X_2d = svd.fit_transform(X_combined)

plt.figure(figsize=(10, 6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df["cluster"], cmap="tab10", s=10)
plt.title(f"KMeans Cluster Visualization (K={best_k})")
plt.xlabel("SVD 1")
plt.ylabel("SVD 2")
plt.grid(True)
plt.show()

"""## title+text combined TF-IDF + K-means"""

import pandas as pd
import glob
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import pairwise_distances_argmin_min

# âœ… å¦‚æœæ²¡è£…è¿‡ kneedï¼Œè¯·å…ˆå®‰è£…ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
!pip install -q kneed

from kneed import KneeLocator

# 1. åˆå¹¶æ‰€æœ‰ CSV å¹¶æ‹¼æ¥ title + text
df = df_final

df["title"] = df["title"].fillna("").astype(str)
df["text"] = df["text"].fillna("").astype(str)
df["text_combined"] = df["title"] + " " + df["text"]
df = df[df["text_combined"].str.len() > 20].reset_index(drop=True)

# 2. TF-IDF å‘é‡åŒ–
vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, stop_words='english')
X = vectorizer.fit_transform(df["text_combined"])

# 3. è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜ Kï¼ˆElbow æ³• + Silhouette Scoreï¼‰
inertias = []
silhouettes = []
ks = list(range(2, 15))

print("\nğŸ“ˆ Calculating Inertia & Silhouette Scores for K from 2 to 14...")
for k in ks:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X)
    inertia = km.inertia_
    silhouette = silhouette_score(X, labels)
    inertias.append(inertia)
    silhouettes.append(silhouette)
    print(f"K = {k}, Inertia = {round(inertia, 2)}, Silhouette = {round(silhouette, 4)}")

# ç”»åŒè½´å›¾ï¼šElbow + Silhouette
fig, ax1 = plt.subplots(figsize=(10, 5))

ax1.set_xlabel("Number of Clusters (K)")
ax1.set_ylabel("Inertia", color="tab:blue")
ax1.plot(ks, inertias, marker="o", color="tab:blue", label="Inertia")
ax1.tick_params(axis="y", labelcolor="tab:blue")

ax2 = ax1.twinx()
ax2.set_ylabel("Silhouette Score", color="tab:green")
ax2.plot(ks, silhouettes, marker="s", linestyle="--", color="tab:green", label="Silhouette Score")
ax2.tick_params(axis="y", labelcolor="tab:green")

plt.title("Elbow Method & Silhouette Score (TF-IDF + Title + Text)")
fig.tight_layout()
plt.grid(True)
plt.show()

# âœ… æ‹ç‚¹è¯†åˆ«ï¼ˆåŸºäº Inertiaï¼‰
kl = KneeLocator(ks, inertias, curve="convex", direction="decreasing")
best_k = kl.elbow
print(f"\nâœ… Optimal K (by Elbow): {best_k}")

"""## text+title combined BERT + KMeans"""

!pip install -q sentence-transformers

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. è¯»å–æ•°æ®
df = df_final
df['title'] = df['title'].fillna('')
df['text'] = df['text'].fillna('')
df['combined'] = df['title'] + ' ' + df['text']

# 2. ç”Ÿæˆå¥å‘é‡ï¼ˆMiniLM éå¸¸å¿«ä¸”æ•ˆæœå¥½ï¼‰
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(df['combined'].tolist(), show_progress_bar=True)

# 3. KMeans èšç±» + silhouette score è¯„ä¼°
k_range = range(2, 15)
scores = []
print("ğŸ“ˆ Calculating silhouette scores:")
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(embeddings)
    score = silhouette_score(embeddings, labels)
    scores.append(score)
    print(f"K={k}, Silhouette Score={round(score, 4)}")

# 4. å¯è§†åŒ– silhouette åˆ†æ•°
plt.plot(k_range, scores, marker='o')
plt.title("Silhouette Score by K")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()

# 5. èšç±»å¹¶ä¿å­˜ç»“æœï¼ˆä½ å¯ä»¥é€‰æœ€ä½³ Kï¼Œæ¯”å¦‚ k=5ï¼‰
best_k = 5
model_km = KMeans(n_clusters=best_k, random_state=42)
df['cluster'] = model_km.fit_predict(embeddings)

# 6. æ¯ç°‡è¾“å‡ºå‡ æ¡ç¤ºä¾‹
print("\nğŸ“Œ æ¯ä¸ªç°‡çš„ä»£è¡¨æ€§è¯„è®ºï¼ˆtitle + å‰ 100 å­— textï¼‰:\n")
for k in range(best_k):
    sample = df[df['cluster'] == k].iloc[0]
    print(f"Cluster {k}:")
    print(f"Title: {sample['title']}")
    print(f"Text: {sample['text'][:100]}...\n")

# 7. é™ç»´å¯è§†åŒ–ï¼ˆç”¨ PCAï¼‰
pca = PCA(n_components=2)
X_2d = pca.fit_transform(embeddings)
plt.figure(figsize=(10,6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df["cluster"], cmap="tab10", s=10)
plt.title("Sentence-BERT Cluster Visualization")
plt.xlabel("PCA-1")
plt.ylabel("PCA-2")
plt.grid(True)
plt.show()

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. è¯»å–æ•°æ®
df = df_final
df['title'] = df['title'].fillna('')
df['text'] = df['text'].fillna('')
df['combined'] = df['title'] + ' ' + df['text']

# 2. ç”Ÿæˆå¥å‘é‡ï¼ˆMiniLM éå¸¸å¿«ä¸”æ•ˆæœå¥½ï¼‰
model = SentenceTransformer('all-mpnet-base-v2')
embeddings = model.encode(df['combined'].tolist(), show_progress_bar=True)

# 3. KMeans èšç±» + silhouette score è¯„ä¼°
k_range = range(2, 15)
scores = []
print("ğŸ“ˆ Calculating silhouette scores:")
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(embeddings)
    score = silhouette_score(embeddings, labels)
    scores.append(score)
    print(f"K={k}, Silhouette Score={round(score, 4)}")

# 4. å¯è§†åŒ– silhouette åˆ†æ•°
plt.plot(k_range, scores, marker='o')
plt.title("Silhouette Score by K")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()

# 5. èšç±»å¹¶ä¿å­˜ç»“æœï¼ˆä½ å¯ä»¥é€‰æœ€ä½³ Kï¼Œæ¯”å¦‚ k=5ï¼‰
best_k = 5
model_km = KMeans(n_clusters=best_k, random_state=42)
df['cluster'] = model_km.fit_predict(embeddings)

# 6. æ¯ç°‡è¾“å‡ºå‡ æ¡ç¤ºä¾‹
print("\nğŸ“Œ æ¯ä¸ªç°‡çš„ä»£è¡¨æ€§è¯„è®ºï¼ˆtitle + å‰ 100 å­— textï¼‰:\n")
for k in range(best_k):
    sample = df[df['cluster'] == k].iloc[0]
    print(f"Cluster {k}:")
    print(f"Title: {sample['title']}")
    print(f"Text: {sample['text'][:100]}...\n")

# 7. é™ç»´å¯è§†åŒ–ï¼ˆç”¨ PCAï¼‰
pca = PCA(n_components=2)
X_2d = pca.fit_transform(embeddings)
plt.figure(figsize=(10,6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df["cluster"], cmap="tab10", s=10)
plt.title("Sentence-BERT Cluster Visualization")
plt.xlabel("PCA-1")
plt.ylabel("PCA-2")
plt.grid(True)
plt.show()

"""## BerTopicèšç±»åˆ†æ"""

# å®‰è£…ä¾èµ–ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ‰§è¡Œï¼‰
!pip install bertopic
!pip install umap-learn

# Step 1: å¯¼å…¥åº“
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# Step 2: è¯»å–æ•°æ®å¹¶åˆå¹¶æ–‡æœ¬
df = pd.read_csv("/content/drive/MyDrive/5153_negative_reviews.csv")
df["title"] = df["title"].fillna("")
df["text"] = df["text"].fillna("")
df["combined"] = df["title"] + " " + df["text"]

# Step 3: åµŒå…¥å‘é‡ï¼ˆä½¿ç”¨å¼ºæ¨¡å‹ mpnetï¼‰
embedding_model = SentenceTransformer("all-mpnet-base-v2")
embeddings = embedding_model.encode(df["combined"], show_progress_bar=True)

# Step 4: åŸå§‹ BERTopic èšç±»
topic_model = BERTopic(embedding_model=embedding_model, verbose=True)
topics, probs = topic_model.fit_transform(df["combined"], embeddings)

# Step 5: ç²¾ç®€ä¸»é¢˜æ•°é‡ï¼ˆä¿ç•™å‰ 25 ä¸ªå¤§ä¸»é¢˜ï¼‰
topic_model = topic_model.reduce_topics(df["combined"], nr_topics=25)
topics_reduced, probs_reduced = topic_model.transform(df["combined"])

# Step 6: æ·»åŠ å›åŸå§‹æ•°æ®
df["topic"] = topics_reduced
df["topic_prob"] = probs_reduced
df.to_csv("bertopic_labeled_reviews_reduced.csv", index=False)

# Step 7: æ˜¾ç¤ºä¸»é¢˜ä¿¡æ¯ï¼ˆå‡ºç°æœ€å¤šçš„ä¸»é¢˜ï¼‰
print("ğŸ“Œ Top 10 Topics Summary:")
topic_info = topic_model.get_topic_info()
print(topic_info.head(10))

# Step 8: æŸ¥çœ‹ Topic 0 çš„å…³é”®è¯
print("\nğŸ” ç¤ºä¾‹ï¼šTopic 0 çš„å…³é”®è¯")
print(topic_model.get_topic(0))

# Step 9: æ¯ä¸ªä¸»é¢˜çš„ä¸€æ¡ä»£è¡¨è¯„è®ºï¼ˆæ¦‚ç‡æœ€é«˜ï¼‰
print("\nğŸ“ æ¯ä¸ªä¸»é¢˜çš„ä»£è¡¨è¯„è®ºï¼š\n")
for topic_num in topic_info["Topic"].tolist()[:10]:
    if topic_num == -1:
        continue
    top_doc_idx = df[df["topic"] == topic_num]["topic_prob"].idxmax()
    print(f"Topic {topic_num}: {topic_model.get_topic(topic_num)[:5]}")
    print(f"  â†’ {df.loc[top_doc_idx, 'combined'][:150]}...\n")

# Step 10: å¯è§†åŒ–äº¤äº’å›¾ï¼ˆå»ºè®®åœ¨ Colab ä¸­æ‰“å¼€ï¼‰
topic_model.visualize_topics()

# å®‰è£…ä¾èµ–ï¼ˆç¬¬ä¸€æ¬¡è¿è¡Œæ—¶æ‰§è¡Œï¼‰
!pip install bertopic
!pip install umap-learn

# Step 1: å¯¼å…¥åº“
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# Step 2: è¯»å–æ•°æ®å¹¶åˆå¹¶æ–‡æœ¬
df = pd.read_csv("/content/drive/MyDrive/5153_negative_reviews.csv")
df["title"] = df["title"].fillna("")
df["text"] = df["text"].fillna("")
df["combined"] = df["title"] + " " + df["text"]

# Step 3: åµŒå…¥å‘é‡ï¼ˆä½¿ç”¨å¼ºæ¨¡å‹ mpnetï¼‰
embedding_model = SentenceTransformer("all-mpnet-base-v2")
embeddings = embedding_model.encode(df["combined"], show_progress_bar=True)

# Step 4: åŸå§‹ BERTopic èšç±»
topic_model = BERTopic(embedding_model=embedding_model, verbose=True)
topics, probs = topic_model.fit_transform(df["combined"], embeddings)

# Step 5: ç²¾ç®€ä¸»é¢˜æ•°é‡ï¼ˆä¿ç•™å‰ 25 ä¸ªä¸»é¢˜ï¼‰
topic_model = topic_model.reduce_topics(df["combined"], nr_topics=25)

# âš ï¸ å†æ¬¡è°ƒç”¨ transform å¾—åˆ°ç²¾ç®€åçš„ä¸»é¢˜ç¼–å·
topics_reduced, probs_reduced = topic_model.transform(df["combined"])

# Step 6: ç”¨ç²¾ç®€ç»“æœæ›´æ–°å› DataFrame
df["topic"] = topics_reduced
df["topic_prob"] = probs_reduced

# âœ… Step 7: å¯¼å‡ºæƒ³è¦çš„å­—æ®µï¼ˆç¡®ä¿åªåŒ…å«ç²¾ç®€ä¸»é¢˜ç¼–å·ï¼‰
cols_to_export = ["review_id", "title", "text", "topic", "topic_prob"]
df_export = df[cols_to_export]
df_export.to_csv("bertopic_clustered_reviews_filtered.csv", index=False)

# ï¼ˆå¯é€‰ï¼‰ä¸‹è½½
from google.colab import files
files.download("bertopic_clustered_reviews_filtered.csv")

# Step 8: æ‰“å°ä¸»é¢˜æ‘˜è¦ï¼ˆç¡®è®¤æ˜¯25ä¸ªï¼‰
print("ğŸ“Œ Top Topics Summary (After Reduction):")
topic_info = topic_model.get_topic_info()
print(topic_info.head(10))

# Step 9: æ¯ä¸ªä¸»é¢˜çš„ä¸€æ¡ä»£è¡¨è¯„è®ºï¼ˆæ¦‚ç‡æœ€é«˜ï¼‰
print("\nğŸ“ æ¯ä¸ªä¸»é¢˜çš„ä»£è¡¨å…³é”®è¯ä¸ä»£è¡¨è¯„è®ºï¼š\n")

for topic_num in topic_info["Topic"].tolist():
    if topic_num == -1:
        continue  # è·³è¿‡å™ªéŸ³ç±»
    # è·å–å…³é”®è¯
    top_words = topic_model.get_topic(topic_num)[:5]
    keyword_list = [w[0] for w in top_words]

    # è·å–ä»£è¡¨è¯„è®ºï¼ˆæœ€é«˜ topic_probï¼‰
    top_idx = df[df["topic"] == topic_num]["topic_prob"].idxmax()
    top_text = df.loc[top_idx, "combined"][:200].replace("\n", " ").strip()

    print(f"ğŸ”¹ Topic {topic_num}: {', '.join(keyword_list)}")
    print(f"   â†’ {top_text}\n")
# Step 10: å¯è§†åŒ–ï¼ˆå»ºè®® Colab ä¸­æŸ¥çœ‹ï¼‰
topic_model.visualize_topics()

# Step 11: å¯¼å‡ºåŒ…å«åŸå§‹å­—æ®µ + ç²¾ç®€åçš„ä¸»é¢˜ç»“æœ
cols_to_export = ["review_id", "title", "text", "topic", "topic_prob"]
df_export = df[cols_to_export]

# ä¿å­˜ä¸º CSV æ–‡ä»¶
df_export.to_csv("/content/drive/MyDrive/bertopic_clustered_reviews_filtered.csv", index=False)