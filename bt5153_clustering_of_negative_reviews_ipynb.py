# -*- coding: utf-8 -*-
"""“BT5153_Clustering_of_Negative_Reviews.ipynb”的副本

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14_I-qT9omJgFs6-_qQqZ2pOBXxz4iBRH
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

json_path = '/content/drive/MyDrive/transformer_predictions.json'
df_test = pd.read_json(json_path, lines=True)

# 查看数据结构
print(df_test.info())
print(df_test.head())

df_reviews = pd.read_csv('/content/drive/MyDrive/final_balanced_reviews.csv')

print(df_reviews.info())
print(df_reviews.head())

df_merged = pd.merge(df_test, df_reviews, on='review_id', how='left')
df_final = df_merged[['review_id', 'text','title', 'transformer_pred']]
df_final = df_final[df_final['transformer_pred'] == 0]
df = df_final

!pip install kneed

df_final.to_csv("/content/drive/MyDrive/5153_negative_reviews.csv")

"""## Only text TF-IDF + K-means"""

from sklearn.metrics import silhouette_score
import pandas as pd
import glob
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import pairwise_distances_argmin_min
from kneed import KneeLocator

# 1. 加载数据
df = df_final

# 2. 准备字段（只保留 text）
df["text"] = df["text"].fillna("").astype(str)
df = df[df["text"].str.len() > 20].reset_index(drop=True)

# 3. 用 text 做 TF-IDF 向量化
vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, stop_words='english', max_features=5000)
X_text = vectorizer.fit_transform(df["text"])

# 4. Elbow 法 + Silhouette Score 找最佳 K
inertias = []
silhouettes = []
ks = range(2, 15)
print("\n📈 Calculating inertia & silhouette for different K values...")
for k in ks:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X_text)
    inertias.append(km.inertia_)
    score = silhouette_score(X_text, labels)
    silhouettes.append(score)
    print(f"K={k}, Inertia={round(km.inertia_, 2)}, Silhouette={round(score, 4)}")

# 双轴图：Inertia + Silhouette
fig, ax1 = plt.subplots(figsize=(10, 5))

color1 = "tab:blue"
ax1.set_xlabel("Number of Clusters (K)")
ax1.set_ylabel("Inertia", color=color1)
ax1.plot(ks, inertias, marker='o', color=color1)
ax1.tick_params(axis="y", labelcolor=color1)

ax2 = ax1.twinx()
color2 = "tab:green"
ax2.set_ylabel("Silhouette Score", color=color2)
ax2.plot(ks, silhouettes, marker='s', linestyle='--', color=color2)
ax2.tick_params(axis="y", labelcolor=color2)

plt.title("Elbow Method + Silhouette Score (Text Only)")
plt.grid(True)
plt.show()

# 拐点检测（仍用 Inertia 找 best_k）
from kneed import KneeLocator
kl = KneeLocator(ks, inertias, curve="convex", direction="decreasing")
best_k = kl.elbow
print(f"\n✅ Optimal K from Elbow: {best_k}")

# 5. 聚类
model = KMeans(n_clusters=best_k, random_state=42)
df["cluster"] = model.fit_predict(X_text)

# 6. 每个 cluster 的关键词（取前 10 个）
terms = vectorizer.get_feature_names_out()
order_centroids = model.cluster_centers_.argsort()[:, ::-1]

print("\n📌 每个簇的前 10 个关键词（只用 text）：\n")
for i in range(best_k):
    top_words = [terms[ind] for ind in order_centroids[i, :10]]
    print(f"Cluster {i}: {', '.join(top_words)}")

# 7. 每个 cluster 的代表评论（最靠近中心）
print("\n📝 每个 cluster 的代表评论：\n")
closest, _ = pairwise_distances_argmin_min(model.cluster_centers_, X_text)
for i, idx in enumerate(closest):
    print(f"\nCluster {i} Example Review:\n{df.iloc[idx]['text'][:400]}...\n")

# 8. 可视化降维
svd = TruncatedSVD(n_components=2, random_state=42)
X_svd = svd.fit_transform(X_text)

plt.figure(figsize=(10, 6))
plt.scatter(X_svd[:, 0], X_svd[:, 1], c=df["cluster"], cmap="tab10", s=10)
plt.title(f"KMeans Cluster Visualization (Text Only, K={best_k})")
plt.xlabel("SVD 1")
plt.ylabel("SVD 2")
plt.grid(True)
plt.show()

# 9. 每个 cluster 抽样 50 条文本（可选）
sampled_df = df.groupby("cluster").apply(lambda x: x.sample(n=min(50, len(x)), random_state=42)).reset_index(drop=True)
sampled_df.to_csv("text_only_cluster_samples.csv", index=False)
print("\n📦 Sampled 50 comments per cluster saved to: text_only_cluster_samples.csv")

"""## text和title分开encoding, TF-IDF + K-means"""

import pandas as pd
import glob
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import pairwise_distances_argmin_min, silhouette_score  # ✅ 加上 silhouette_score
from scipy.sparse import hstack
from kneed import KneeLocator

df = df_final

df["title"] = df["title"].fillna("").astype(str)
df["text"] = df["text"].fillna("").astype(str)
df = df[df["text"].str.len() > 20].reset_index(drop=True)

# 2. 分开向量化 title 和 text（保留高频重要词）
vectorizer_title = TfidfVectorizer(max_df=0.8, min_df=3, stop_words='english', max_features=2000)
vectorizer_text = TfidfVectorizer(max_df=0.9, min_df=5, stop_words='english', max_features=5000)

X_title = vectorizer_title.fit_transform(df["title"])
X_text = vectorizer_text.fit_transform(df["text"])

# 3. 合并 title 和 text 的特征
X_combined = hstack([X_title, X_text])

# 4. Elbow 法找最佳 K
inertias = []
silhouette_scores = []  # ✅ 存储每个 k 的轮廓系数
ks = range(2, 15)
print("\n📈 Calculating inertia & silhouette for different K values...")
for k in ks:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X_combined)
    inertia = km.inertia_
    inertias.append(inertia)

    # ✅ 计算 silhouette（样本数 >= k+1，否则可能报错）
    if len(df) > k:
        score = silhouette_score(X_combined, labels)
        silhouette_scores.append(score)
        print(f"K={k}, Inertia={round(inertia, 2)}, Silhouette={round(score, 4)}")
    else:
        silhouette_scores.append(None)
        print(f"K={k}, Inertia={round(inertia, 2)}, Silhouette=N/A")

# 画 Elbow 图 + Silhouette Score 图
fig, ax1 = plt.subplots(figsize=(10, 5))
ax1.plot(ks, inertias, marker='o', color='blue', label='Inertia')
ax1.set_xlabel("Number of Clusters (K)")
ax1.set_ylabel("Inertia", color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1.set_title("Elbow Method & Silhouette Score")

ax2 = ax1.twinx()
ax2.plot(ks, silhouette_scores, marker='s', color='green', label='Silhouette Score')
ax2.set_ylabel("Silhouette Score", color='green')
ax2.tick_params(axis='y', labelcolor='green')

plt.grid(True)
plt.show()

# 自动识别拐点
kl = KneeLocator(ks, inertias, curve="convex", direction="decreasing")
best_k = kl.elbow
print(f"\n✅ Optimal K detected by Elbow: {best_k}")

# 5. 用最佳 K 聚类
model = KMeans(n_clusters=best_k, random_state=42)
df["cluster"] = model.fit_predict(X_combined)

# 6. 输出每个 cluster 的前 10 个关键词（仅 text 部分）
terms = vectorizer_text.get_feature_names_out()
order_centroids = model.cluster_centers_[:, X_title.shape[1]:].argsort()[:, ::-1]

print("\n📌 每个簇的前 10 个关键词（仅基于正文）\n")
for i in range(best_k):
    top_words = [terms[ind] for ind in order_centroids[i, :10]]
    print(f"Cluster {i}: {', '.join(top_words)}")

# 7. 每个 cluster 的代表评论（最靠近中心）
print("\n📝 每个 cluster 的代表评论：\n")
closest, _ = pairwise_distances_argmin_min(model.cluster_centers_, X_combined)
for i, idx in enumerate(closest):
    print(f"\nCluster {i} Example Review:\n{df.iloc[idx]['title']} | {df.iloc[idx]['text'][:400]}...\n")

# 8. 可视化：TruncatedSVD 降到 2 维
svd = TruncatedSVD(n_components=2, random_state=42)
X_2d = svd.fit_transform(X_combined)

plt.figure(figsize=(10, 6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df["cluster"], cmap="tab10", s=10)
plt.title(f"KMeans Cluster Visualization (K={best_k})")
plt.xlabel("SVD 1")
plt.ylabel("SVD 2")
plt.grid(True)
plt.show()

"""## title+text combined TF-IDF + K-means"""

import pandas as pd
import glob
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import pairwise_distances_argmin_min

# ✅ 如果没装过 kneed，请先安装（只需一次）
!pip install -q kneed

from kneed import KneeLocator

# 1. 合并所有 CSV 并拼接 title + text
df = df_final

df["title"] = df["title"].fillna("").astype(str)
df["text"] = df["text"].fillna("").astype(str)
df["text_combined"] = df["title"] + " " + df["text"]
df = df[df["text_combined"].str.len() > 20].reset_index(drop=True)

# 2. TF-IDF 向量化
vectorizer = TfidfVectorizer(max_df=0.9, min_df=5, stop_words='english')
X = vectorizer.fit_transform(df["text_combined"])

# 3. 自动寻找最优 K（Elbow 法 + Silhouette Score）
inertias = []
silhouettes = []
ks = list(range(2, 15))

print("\n📈 Calculating Inertia & Silhouette Scores for K from 2 to 14...")
for k in ks:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X)
    inertia = km.inertia_
    silhouette = silhouette_score(X, labels)
    inertias.append(inertia)
    silhouettes.append(silhouette)
    print(f"K = {k}, Inertia = {round(inertia, 2)}, Silhouette = {round(silhouette, 4)}")

# 画双轴图：Elbow + Silhouette
fig, ax1 = plt.subplots(figsize=(10, 5))

ax1.set_xlabel("Number of Clusters (K)")
ax1.set_ylabel("Inertia", color="tab:blue")
ax1.plot(ks, inertias, marker="o", color="tab:blue", label="Inertia")
ax1.tick_params(axis="y", labelcolor="tab:blue")

ax2 = ax1.twinx()
ax2.set_ylabel("Silhouette Score", color="tab:green")
ax2.plot(ks, silhouettes, marker="s", linestyle="--", color="tab:green", label="Silhouette Score")
ax2.tick_params(axis="y", labelcolor="tab:green")

plt.title("Elbow Method & Silhouette Score (TF-IDF + Title + Text)")
fig.tight_layout()
plt.grid(True)
plt.show()

# ✅ 拐点识别（基于 Inertia）
kl = KneeLocator(ks, inertias, curve="convex", direction="decreasing")
best_k = kl.elbow
print(f"\n✅ Optimal K (by Elbow): {best_k}")

"""## text+title combined BERT + KMeans"""

!pip install -q sentence-transformers

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. 读取数据
df = df_final
df['title'] = df['title'].fillna('')
df['text'] = df['text'].fillna('')
df['combined'] = df['title'] + ' ' + df['text']

# 2. 生成句向量（MiniLM 非常快且效果好）
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(df['combined'].tolist(), show_progress_bar=True)

# 3. KMeans 聚类 + silhouette score 评估
k_range = range(2, 15)
scores = []
print("📈 Calculating silhouette scores:")
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(embeddings)
    score = silhouette_score(embeddings, labels)
    scores.append(score)
    print(f"K={k}, Silhouette Score={round(score, 4)}")

# 4. 可视化 silhouette 分数
plt.plot(k_range, scores, marker='o')
plt.title("Silhouette Score by K")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()

# 5. 聚类并保存结果（你可以选最佳 K，比如 k=5）
best_k = 5
model_km = KMeans(n_clusters=best_k, random_state=42)
df['cluster'] = model_km.fit_predict(embeddings)

# 6. 每簇输出几条示例
print("\n📌 每个簇的代表性评论（title + 前 100 字 text）:\n")
for k in range(best_k):
    sample = df[df['cluster'] == k].iloc[0]
    print(f"Cluster {k}:")
    print(f"Title: {sample['title']}")
    print(f"Text: {sample['text'][:100]}...\n")

# 7. 降维可视化（用 PCA）
pca = PCA(n_components=2)
X_2d = pca.fit_transform(embeddings)
plt.figure(figsize=(10,6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df["cluster"], cmap="tab10", s=10)
plt.title("Sentence-BERT Cluster Visualization")
plt.xlabel("PCA-1")
plt.ylabel("PCA-2")
plt.grid(True)
plt.show()

import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 1. 读取数据
df = df_final
df['title'] = df['title'].fillna('')
df['text'] = df['text'].fillna('')
df['combined'] = df['title'] + ' ' + df['text']

# 2. 生成句向量（MiniLM 非常快且效果好）
model = SentenceTransformer('all-mpnet-base-v2')
embeddings = model.encode(df['combined'].tolist(), show_progress_bar=True)

# 3. KMeans 聚类 + silhouette score 评估
k_range = range(2, 15)
scores = []
print("📈 Calculating silhouette scores:")
for k in k_range:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(embeddings)
    score = silhouette_score(embeddings, labels)
    scores.append(score)
    print(f"K={k}, Silhouette Score={round(score, 4)}")

# 4. 可视化 silhouette 分数
plt.plot(k_range, scores, marker='o')
plt.title("Silhouette Score by K")
plt.xlabel("Number of Clusters")
plt.ylabel("Silhouette Score")
plt.grid(True)
plt.show()

# 5. 聚类并保存结果（你可以选最佳 K，比如 k=5）
best_k = 5
model_km = KMeans(n_clusters=best_k, random_state=42)
df['cluster'] = model_km.fit_predict(embeddings)

# 6. 每簇输出几条示例
print("\n📌 每个簇的代表性评论（title + 前 100 字 text）:\n")
for k in range(best_k):
    sample = df[df['cluster'] == k].iloc[0]
    print(f"Cluster {k}:")
    print(f"Title: {sample['title']}")
    print(f"Text: {sample['text'][:100]}...\n")

# 7. 降维可视化（用 PCA）
pca = PCA(n_components=2)
X_2d = pca.fit_transform(embeddings)
plt.figure(figsize=(10,6))
plt.scatter(X_2d[:, 0], X_2d[:, 1], c=df["cluster"], cmap="tab10", s=10)
plt.title("Sentence-BERT Cluster Visualization")
plt.xlabel("PCA-1")
plt.ylabel("PCA-2")
plt.grid(True)
plt.show()

"""## BerTopic聚类分析"""

# 安装依赖（第一次运行时执行）
!pip install bertopic
!pip install umap-learn

# Step 1: 导入库
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# Step 2: 读取数据并合并文本
df = pd.read_csv("/content/drive/MyDrive/5153_negative_reviews.csv")
df["title"] = df["title"].fillna("")
df["text"] = df["text"].fillna("")
df["combined"] = df["title"] + " " + df["text"]

# Step 3: 嵌入向量（使用强模型 mpnet）
embedding_model = SentenceTransformer("all-mpnet-base-v2")
embeddings = embedding_model.encode(df["combined"], show_progress_bar=True)

# Step 4: 原始 BERTopic 聚类
topic_model = BERTopic(embedding_model=embedding_model, verbose=True)
topics, probs = topic_model.fit_transform(df["combined"], embeddings)

# Step 5: 精简主题数量（保留前 25 个大主题）
topic_model = topic_model.reduce_topics(df["combined"], nr_topics=25)
topics_reduced, probs_reduced = topic_model.transform(df["combined"])

# Step 6: 添加回原始数据
df["topic"] = topics_reduced
df["topic_prob"] = probs_reduced
df.to_csv("bertopic_labeled_reviews_reduced.csv", index=False)

# Step 7: 显示主题信息（出现最多的主题）
print("📌 Top 10 Topics Summary:")
topic_info = topic_model.get_topic_info()
print(topic_info.head(10))

# Step 8: 查看 Topic 0 的关键词
print("\n🔍 示例：Topic 0 的关键词")
print(topic_model.get_topic(0))

# Step 9: 每个主题的一条代表评论（概率最高）
print("\n📝 每个主题的代表评论：\n")
for topic_num in topic_info["Topic"].tolist()[:10]:
    if topic_num == -1:
        continue
    top_doc_idx = df[df["topic"] == topic_num]["topic_prob"].idxmax()
    print(f"Topic {topic_num}: {topic_model.get_topic(topic_num)[:5]}")
    print(f"  → {df.loc[top_doc_idx, 'combined'][:150]}...\n")

# Step 10: 可视化交互图（建议在 Colab 中打开）
topic_model.visualize_topics()

# 安装依赖（第一次运行时执行）
!pip install bertopic
!pip install umap-learn

# Step 1: 导入库
import pandas as pd
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

# Step 2: 读取数据并合并文本
df = pd.read_csv("/content/drive/MyDrive/5153_negative_reviews.csv")
df["title"] = df["title"].fillna("")
df["text"] = df["text"].fillna("")
df["combined"] = df["title"] + " " + df["text"]

# Step 3: 嵌入向量（使用强模型 mpnet）
embedding_model = SentenceTransformer("all-mpnet-base-v2")
embeddings = embedding_model.encode(df["combined"], show_progress_bar=True)

# Step 4: 原始 BERTopic 聚类
topic_model = BERTopic(embedding_model=embedding_model, verbose=True)
topics, probs = topic_model.fit_transform(df["combined"], embeddings)

# Step 5: 精简主题数量（保留前 25 个主题）
topic_model = topic_model.reduce_topics(df["combined"], nr_topics=25)

# ⚠️ 再次调用 transform 得到精简后的主题编号
topics_reduced, probs_reduced = topic_model.transform(df["combined"])

# Step 6: 用精简结果更新回 DataFrame
df["topic"] = topics_reduced
df["topic_prob"] = probs_reduced

# ✅ Step 7: 导出想要的字段（确保只包含精简主题编号）
cols_to_export = ["review_id", "title", "text", "topic", "topic_prob"]
df_export = df[cols_to_export]
df_export.to_csv("bertopic_clustered_reviews_filtered.csv", index=False)

# （可选）下载
from google.colab import files
files.download("bertopic_clustered_reviews_filtered.csv")

# Step 8: 打印主题摘要（确认是25个）
print("📌 Top Topics Summary (After Reduction):")
topic_info = topic_model.get_topic_info()
print(topic_info.head(10))

# Step 9: 每个主题的一条代表评论（概率最高）
print("\n📝 每个主题的代表关键词与代表评论：\n")

for topic_num in topic_info["Topic"].tolist():
    if topic_num == -1:
        continue  # 跳过噪音类
    # 获取关键词
    top_words = topic_model.get_topic(topic_num)[:5]
    keyword_list = [w[0] for w in top_words]

    # 获取代表评论（最高 topic_prob）
    top_idx = df[df["topic"] == topic_num]["topic_prob"].idxmax()
    top_text = df.loc[top_idx, "combined"][:200].replace("\n", " ").strip()

    print(f"🔹 Topic {topic_num}: {', '.join(keyword_list)}")
    print(f"   → {top_text}\n")
# Step 10: 可视化（建议 Colab 中查看）
topic_model.visualize_topics()

# Step 11: 导出包含原始字段 + 精简后的主题结果
cols_to_export = ["review_id", "title", "text", "topic", "topic_prob"]
df_export = df[cols_to_export]

# 保存为 CSV 文件
df_export.to_csv("/content/drive/MyDrive/bertopic_clustered_reviews_filtered.csv", index=False)